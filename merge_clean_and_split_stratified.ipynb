{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d94c506",
   "metadata": {},
   "source": [
    "This notebook will perform our temporal train/test split of the dataset, after merging the two existing datasets into one and filtering out entries in the Disaster Declaration Summaries (DDS) that do not exist in the Mission Assignments (MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f2afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff5fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fbcc1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_filepath = 'mission_assignments.parquet'\n",
    "dds_filepath = 'disaster_declaration_summaries.parquet'\n",
    "train_filepath = 'combined_training_set_nontime.parquet'\n",
    "test_filepath = 'combined_test_set_nontime.parquet'\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "669a1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68485, 28) (40340, 39)\n"
     ]
    }
   ],
   "source": [
    "# Load initial datasets\n",
    "\n",
    "df_dds = pd.read_parquet(dds_filepath)\n",
    "df_ma = pd.read_parquet(ma_filepath)\n",
    "print(df_dds.shape, df_ma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc629367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       R\n",
       "1       R\n",
       "2       R\n",
       "3    None\n",
       "4    None\n",
       "5    None\n",
       "6    None\n",
       "7    None\n",
       "8    None\n",
       "9       R\n",
       "Name: designatedIncidentTypes, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dds['designatedIncidentTypes'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02fe4f",
   "metadata": {},
   "source": [
    "Adding lists and dictionaries for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638291b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to convert state/territory designators to full word strings\n",
    "state_dict = {'AL':'Alabama','AK':'Alaska','AZ':'Arizona','AR':'Arkansas','CA':'California','CO':'Colorado','CT':'Connecticut',\n",
    "             'DE':'Delaware', 'FL':'Florida','GA':'Georgia','HI':'Hawaii','ID':'Idaho','IL':'Illinois','IN':'Indiana','IA':'Iowa',\n",
    "             'KS':'Kansas','KY':'Kentucky','LA':'Louisiana','ME':'Maine','MD':'Maryland','MA':'Massachusetts','MI':'Michigan',\n",
    "             'MN':'Minnesota','MS':'Mississippi','MO':'Missouri','MT':'Montana','NE':'Nebraska','NV':'Nevada','NH':'New Hampshire',\n",
    "             'NM':'New Mexico','NY':'New York','NJ':'New Jersey','NC':'North Carolina','ND':'North Dakota','OH':'Ohio',\n",
    "             'OK':'Oklahoma','OR':'Oregon','PA':'Pennsylvania','RI':'Rhode Island','SC':'South Carolina','SD':'South Dakota',\n",
    "             'TN':'Tennessee','TX':'Texas','UT':'Utah','VT':'Vermont','VA_state':'Virginia','WA':'Washington','WV':'West Virginia',\n",
    "             'WI':'Wisconsin','WY':'Wyoming','DC':'Washington, DC','GU':'Guam','PR':'Puerto Rico','AS':'American Samoa',\n",
    "             'MP':'Northern Mariana Islands','FM':'Federated States of Micronesia','MH':'Marshall Islands','PW':'Palau'}\n",
    "\n",
    "state_list = ['AL','AZ','AR','CA','CO','CT','DE', 'FL','GA','ID','IL','IN','IA','KS','KY','LA','ME','MD','MA','MI',\n",
    "             'MN','MS','MO','MT','NE','NV','NH','NM','NY','NJ','NC','ND','OH','OK','OR','PA','RI','SC','SD',\n",
    "             'TN','TX','UT','VT','VA','WA','WV','WI','WY']\n",
    "\n",
    "#set using only natural disasters that could be potentially caused by climate change\n",
    "natural_disaster = ['Fire','Flood','Severe Storm','Straight-Line Winds','Winter Storm','Hurricane','Tornado','Tropical Storm',\n",
    "                   'Mud/Landslide','Snowstorm','Coastal Storm','Severe Ice Storm','Typhoon','Freezing','Drought','Fishing Losses',\n",
    "                   'Tropical Depression']\n",
    "\n",
    "#manmade or other disasters that would not be caused by climate change\n",
    "nonweather_disaster = ['Earthquake','Other','Biological','Dam/Levee Break','Volcanic Eruption','Toxic Substances','Chemical',\n",
    "                      'Terrorist','Human Cause','Tsunami','Civil Unrest','Nuclear','Explosion','Tidal Wave']\n",
    "\n",
    "#dictionary to convert disaster codes to strings representing each type of disaster\n",
    "disaster_dict = {'0':'Not applicable','1':'Explosion','2':'Straight-Line Winds','3':'Tidal Wave','4':'Tropical Storm',\n",
    "                '5':'Winter Storm','A':'Tsunami','B':'Biological','C':'Coastal Storm','D':'Drought','E':'Earthquake',\n",
    "                'F':'Flood','G':'Freezing','H':'Hurricane','I':'Terrorist','J':'Typhoon','K':'Dam/Levee Break','L':'Chemical',\n",
    "                'M':'Mud/Landslide','N':'Nuclear','O':'Severe Ice Storm','P':'Fishing Losses','Q':'Crop Losses','R':'Fire',\n",
    "                'S':'Snowstorm','T':'Tornado','U':'Civil Unrest', 'V':'Volcanic Eruption','W':'Severe Storm','X':'Toxic Substances',\n",
    "                'Y':'Human Cause','Z':'Other', '8':'Tropical Depression'}\n",
    "\n",
    "agencyid_dict = {'CISA':'DHS-CISA','DHSMGMT':'DHS-MGMT','USDANRCS':'USDA-NRCS','GSA-':'GSA','VA-':'VA','EPA-':'EPA','DOT-':'DOT',\n",
    "                'CNCS-':'CNCS','FCC-':'FCC','DOED':'DOE','DHUD':'HUD','DOD-':'DOD','VA -':'VA','USDAOCIO':'USDA-OCIO','FPS':'DHS-FPS',\n",
    "                'TSA':'DHS-TSA','ICE':'DHS-ICE','USCIS':'DHS-CIS','DLA':'DOD-DLA','CBP':'DHS-CBP','NPS':'DOI-NPS','NPPD':'DHS-CISA',\n",
    "                'CDC':'HHS-CDC','USAF':'DOD-USAF','OSHA':'DOL-OSHA','DHS-MGT':'DHS-MGMT','USGS':'DOI-USGS','USCG':'DHS-USCG',\n",
    "                'USDJ':'DOJ','DHS-MGA':'DHS-IA','FLETC':'DHS-FLETC','DHS-FLET':'DHS-FLETC','USFS':'USDA-FS','HHS -PSC':'HHS-PSC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19463f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39915     None\n",
       "2356      None\n",
       "41130        F\n",
       "59627        W\n",
       "68477     None\n",
       "4596      None\n",
       "47342        T\n",
       "35694     None\n",
       "17321      5,S\n",
       "63078    W,F,T\n",
       "49362     None\n",
       "6821      None\n",
       "55723        W\n",
       "51986      R,W\n",
       "34427     None\n",
       "64659      W,F\n",
       "2178      None\n",
       "55140        F\n",
       "10423     None\n",
       "53010        W\n",
       "65695        W\n",
       "45847     None\n",
       "18730     None\n",
       "37673     None\n",
       "33205        Z\n",
       "20807     None\n",
       "61318        W\n",
       "19623     None\n",
       "34070     None\n",
       "15466     None\n",
       "Name: designatedIncidentTypes, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dds['designatedIncidentTypes'].sample(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff33a8cd",
   "metadata": {},
   "source": [
    "Data cleaning for MA includes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723bc4d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/mh2mgzhn1191h4zq_5d03wfc0000gn/T/ipykernel_54393/1173886563.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_ma['supportFunction'].fillna(value=0,inplace=True)\n",
      "/var/folders/15/mh2mgzhn1191h4zq_5d03wfc0000gn/T/ipykernel_54393/1173886563.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_ma['agencyId'].replace(agencyid_dict,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7044, 12)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ma=df_ma[(df_ma['declarationType']!='SU')&(df_ma['maAmendNumber']==0)&(df_ma['supportFunction']<=15)]\n",
    "\n",
    "df_ma['supportFunction'].fillna(value=0,inplace=True)\n",
    "\n",
    "# df_ma['stt'].replace({'VA':'VA_state'},inplace=True)\n",
    "\n",
    "df_ma['agencyId'].replace(agencyid_dict,inplace=True)\n",
    "\n",
    "column_list_ma = ['incidentId','stt','incidentType','region','maType','maPriority','supportFunction','agencyId', 'maId',\n",
    "              'declarationType', 'assistanceRequested', 'statementOfWork']\n",
    "df_ma = df_ma.reindex(columns=column_list_ma)\n",
    "\n",
    "df_ma.drop_duplicates(inplace=True)\n",
    "\n",
    "df_ma.rename(columns={'incidentType': 'incidentTypeMA'},\n",
    "             inplace=True)\n",
    "\n",
    "# df_ma.drop(columns=[\n",
    "#     'lastRefresh',\n",
    "#     'hash',\n",
    "#     'id',],\n",
    "#     inplace=True)\n",
    "\n",
    "df_ma.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a342595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agencyId\n",
       "DOD          743\n",
       "GSA          485\n",
       "HHS          411\n",
       "EPA          360\n",
       "COE-SAD      348\n",
       "            ... \n",
       "DC-CSOSA       1\n",
       "DOC-NTIA       1\n",
       "DOC-BIS        1\n",
       "USDA-OCIO      1\n",
       "USDA-OCP       1\n",
       "Name: count, Length: 109, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ma['agencyId'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd99b13",
   "metadata": {},
   "source": [
    "Data cleaning for DDS includes keeping of specific columns and filtering of year and declaration type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3045fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26041, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/mh2mgzhn1191h4zq_5d03wfc0000gn/T/ipykernel_54393/1474806472.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_dds['designatedIncidentTypes'].fillna(df_dds['incidentType'], inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1123, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select columns necessary for data analysis, add empty columns for each natural disaster type\n",
    "\n",
    "column_list_dds = ['femaDeclarationString','state','incidentType','incidentBeginDate','fipsStateCode','region',\n",
    "               'designatedIncidentTypes','declarationTitle', 'incidentId','declarationType']\n",
    "\n",
    "df_dds = df_dds.reindex(\n",
    "    columns=column_list_dds,\n",
    "    fill_value=0)\n",
    "\n",
    "# Add time information to DDS\n",
    "\n",
    "df_dds['incidentBeginDate']=pd.to_datetime(df_dds['incidentBeginDate'])\n",
    "df_dds['year'] = df_dds['incidentBeginDate'].dt.year\n",
    "df_dds['month'] = df_dds['incidentBeginDate'].dt.month\n",
    "df_dds['day'] = df_dds['incidentBeginDate'].dt.day\n",
    "\n",
    "# Filter out values before 2012\n",
    "\n",
    "df_dds=df_dds[(df_dds['year']>=2012) & (df_dds['declarationType']!='FM')]\n",
    "# df_dds.drop(columns=[\n",
    "#     'lastRefresh',\n",
    "#     'hash',\n",
    "#     'id',],\n",
    "#     inplace=True)\n",
    "print(df_dds.shape)\n",
    "\n",
    "#ensures that incident type is reflected in designated incident types\n",
    "df_dds['designatedIncidentTypes'].fillna(df_dds['incidentType'], inplace = True)\n",
    "\n",
    "df_dds.drop_duplicates(inplace=True)\n",
    "df_dds.reset_index(inplace = True,\n",
    "                   drop=True)\n",
    "\n",
    "df_dds.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0fb61c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['femaDeclarationString',\n",
       " 'state',\n",
       " 'incidentType',\n",
       " 'incidentBeginDate',\n",
       " 'fipsStateCode',\n",
       " 'region',\n",
       " 'designatedIncidentTypes',\n",
       " 'declarationTitle',\n",
       " 'incidentId',\n",
       " 'declarationType',\n",
       " 'year',\n",
       " 'month',\n",
       " 'day']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dds_column_list = df_dds.columns.to_list()\n",
    "dds_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8745ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['incidentId',\n",
       " 'state',\n",
       " 'incidentTypeMA',\n",
       " 'region',\n",
       " 'maType',\n",
       " 'maPriority',\n",
       " 'supportFunction',\n",
       " 'agencyId',\n",
       " 'maId',\n",
       " 'declarationType',\n",
       " 'assistanceRequested',\n",
       " 'statementOfWork']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ma.rename(columns={'stt':'state'},inplace=True)\n",
    "ma_column_list = df_ma.columns.to_list()\n",
    "ma_column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7dfdd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 326\n"
     ]
    }
   ],
   "source": [
    "print(df_dds['incidentId'].nunique(), df_ma['incidentId'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df07c2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['declarationType', 'state', 'region', 'incidentId']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlapping_columns = list(set(ma_column_list).intersection(set(dds_column_list)))\n",
    "overlapping_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "154464a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_disaster_combined=df_ma.merge(\n",
    "    df_dds, \n",
    "    how='left',\n",
    "    on=overlapping_columns,\n",
    "    validate='m:m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6c74f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7699, 21)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_disaster_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6bcb66d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7699, 21)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_disaster_combined.drop_duplicates(inplace=True)\n",
    "MA_disaster_combined.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f07b7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/15/mh2mgzhn1191h4zq_5d03wfc0000gn/T/ipykernel_54393/1635916619.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  MA_disaster_combined['designatedIncidentTypes'].fillna(MA_disaster_combined['incidentTypeMA'], inplace = True)\n"
     ]
    }
   ],
   "source": [
    "MA_disaster_combined['designatedIncidentTypes'].fillna(MA_disaster_combined['incidentTypeMA'], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51b82142",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_disaster_combined['designatedIncidentTypes'] = MA_disaster_combined['designatedIncidentTypes'].str.split(',').apply(\n",
    "    lambda lst: [s.strip() for s in lst] if isinstance(lst, list) else lst).apply(\n",
    "    lambda lst: [disaster_dict.get(s, s) for s in lst] if isinstance(lst, list) else lst).apply(\n",
    "    lambda lst: ','.join(lst) if isinstance(lst, list) else str(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90d22388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incidentTypeMA</th>\n",
       "      <th>designatedIncidentTypes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7481</th>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Tropical Storm,Mud/Landslide,Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6993</th>\n",
       "      <td>Tropical Depression</td>\n",
       "      <td>Tropical Depression,Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1810</th>\n",
       "      <td>Fire</td>\n",
       "      <td>Fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3874</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Straight-Line Winds,Tropical Storm,Tropical De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6166</th>\n",
       "      <td>Flood</td>\n",
       "      <td>Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3641</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6453</th>\n",
       "      <td>Flood</td>\n",
       "      <td>Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1183</th>\n",
       "      <td>Flood</td>\n",
       "      <td>Flood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1117</th>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>Severe Storm</td>\n",
       "      <td>Severe Storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6947</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Severe Storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm,Severe Storm,Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm,Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>Coastal Storm</td>\n",
       "      <td>Severe Storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2283</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6984</th>\n",
       "      <td>Tropical Depression</td>\n",
       "      <td>Tropical Depression,Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3205</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7335</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm,Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>Hurricane</td>\n",
       "      <td>Hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3206</th>\n",
       "      <td>Biological</td>\n",
       "      <td>Biological</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>Tropical Storm</td>\n",
       "      <td>Tropical Storm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           incidentTypeMA                            designatedIncidentTypes\n",
       "7481            Hurricane                 Tropical Storm,Mud/Landslide,Flood\n",
       "6993  Tropical Depression                      Tropical Depression,Hurricane\n",
       "2101           Biological                                         Biological\n",
       "1810                 Fire                                               Fire\n",
       "3874           Biological                                         Biological\n",
       "4673            Hurricane                                          Hurricane\n",
       "53         Tropical Storm  Straight-Line Winds,Tropical Storm,Tropical De...\n",
       "6166                Flood                                              Flood\n",
       "3641           Biological                                         Biological\n",
       "6453                Flood                                              Flood\n",
       "1183                Flood                                              Flood\n",
       "1117            Hurricane                                          Hurricane\n",
       "1335            Hurricane                                          Hurricane\n",
       "2073         Severe Storm                                       Severe Storm\n",
       "6947       Tropical Storm                                     Tropical Storm\n",
       "4131            Hurricane                                       Severe Storm\n",
       "264        Tropical Storm              Tropical Storm,Severe Storm,Hurricane\n",
       "726        Tropical Storm                           Tropical Storm,Hurricane\n",
       "5023        Coastal Storm                                       Severe Storm\n",
       "506        Tropical Storm                                     Tropical Storm\n",
       "2682           Biological                                         Biological\n",
       "2283           Biological                                         Biological\n",
       "482        Tropical Storm                                     Tropical Storm\n",
       "6984  Tropical Depression                      Tropical Depression,Hurricane\n",
       "3205           Biological                                         Biological\n",
       "7577       Tropical Storm                                          Hurricane\n",
       "7335       Tropical Storm                           Tropical Storm,Hurricane\n",
       "1924            Hurricane                                          Hurricane\n",
       "3206           Biological                                         Biological\n",
       "7565       Tropical Storm                                     Tropical Storm"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_disaster_combined[['incidentTypeMA','designatedIncidentTypes']].sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1fff06f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "incidentId                   0\n",
       "state                        0\n",
       "incidentTypeMA               0\n",
       "region                       0\n",
       "maType                       0\n",
       "maPriority                   0\n",
       "supportFunction              0\n",
       "agencyId                     0\n",
       "maId                         0\n",
       "declarationType              0\n",
       "assistanceRequested          0\n",
       "statementOfWork              0\n",
       "femaDeclarationString      156\n",
       "incidentType               156\n",
       "incidentBeginDate          156\n",
       "fipsStateCode              156\n",
       "designatedIncidentTypes      0\n",
       "declarationTitle           156\n",
       "year                       156\n",
       "month                      156\n",
       "day                        156\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_disaster_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f2f3e651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(MA_disaster_combined[MA_disaster_combined['incidentTypeMA'].isna()]['incidentId'].value_counts())\n",
    "print(MA_disaster_combined[MA_disaster_combined['incidentTypeMA'].isna()]['incidentId'].nunique())\n",
    "\n",
    "ids_without_year = MA_disaster_combined[MA_disaster_combined['incidentTypeMA'].isna()]['incidentId'].tolist()\n",
    "ids_without_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c95b9ef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_disaster_combined['incidentId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e400085",
   "metadata": {},
   "outputs": [],
   "source": [
    "MA_disaster_combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "332384d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_disaster_combined['incidentId'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b1f4518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total groups in Train set: 257\n",
      "Total groups in Test set: 64\n",
      "Number of forced rare groups: 4\n"
     ]
    }
   ],
   "source": [
    "split_info = StratifiedShuffleSplit(n_splits=1,\n",
    "                                    test_size=.2,\n",
    "                                    random_state=random_state)\n",
    "\n",
    "grouping_col = 'incidentId'\n",
    "stratifying_col = 'incidentType'\n",
    "\n",
    "temp_df = MA_disaster_combined.drop_duplicates(subset=[grouping_col]).copy()\n",
    "group_stratify_map = temp_df[stratifying_col]\n",
    "\n",
    "group_counts = group_stratify_map.value_counts()\n",
    "rare_stratify_values = group_counts[group_counts == 1].index.tolist()\n",
    "\n",
    "forced_train_group_ids = temp_df[temp_df[stratifying_col].isin(rare_stratify_values)][grouping_col].values\n",
    "safe_groups_df = temp_df[~temp_df[stratifying_col].isin(rare_stratify_values)]\n",
    "safe_group_ids = safe_groups_df[grouping_col].values\n",
    "safe_stratify_map = safe_groups_df[stratifying_col].values\n",
    "\n",
    "train_safe_group_ids, test_group_ids, _, _ = train_test_split(\n",
    "    safe_group_ids,\n",
    "    safe_stratify_map,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    # This split is now safe because all classes in safe_stratify_map have count >= 2\n",
    "    stratify=safe_stratify_map \n",
    ")\n",
    "\n",
    "# 5. Combine the forced rare groups with the safely split training groups\n",
    "final_train_group_ids = np.concatenate([train_safe_group_ids, forced_train_group_ids])\n",
    "\n",
    "# 6. Apply the final masks to the original full DataFrame\n",
    "train_mask = MA_disaster_combined[grouping_col].isin(final_train_group_ids)\n",
    "test_mask = MA_disaster_combined[grouping_col].isin(test_group_ids)\n",
    "\n",
    "df_train = MA_disaster_combined[train_mask]\n",
    "df_test = MA_disaster_combined[test_mask]\n",
    "\n",
    "print(f\"Total groups in Train set: {len(final_train_group_ids)}\")\n",
    "print(f\"Total groups in Test set: {len(test_group_ids)}\")\n",
    "print(f\"Number of forced rare groups: {len(forced_train_group_ids)}\")\n",
    "\n",
    "# group_stratify_map = MA_disaster_combined.drop_duplicates(subset=[grouping_col])[stratifying_col].values\n",
    "# group_ids = MA_disaster_combined.drop_duplicates(subset=[grouping_col])[grouping_col].values\n",
    "\n",
    "\n",
    "\n",
    "# train_group_ids, test_group_ids, _, _ = train_test_split(\n",
    "#     group_ids,\n",
    "#     group_stratify_map,\n",
    "#     test_size=0.2,\n",
    "#     random_state=42,\n",
    "#     stratify=group_stratify_map  # Stratify the group IDs based on S\n",
    "# )\n",
    "\n",
    "# train_mask = MA_disaster_combined[grouping_col].isin(train_group_ids)\n",
    "# test_mask = MA_disaster_combined[grouping_col].isin(test_group_ids)\n",
    "\n",
    "# # D. Final Train/Test DataFrames\n",
    "# df_train = df[train_mask]\n",
    "# df_test = df[test_mask]\n",
    "# # stratify_info = MA_disaster_combined['incidentType'].values\n",
    "\n",
    "# # for train_index, test_index in split_info.split(MA_disaster_combined, stratify_info):\n",
    "# #     # Use .iloc to slice the DataFrame based on indices\n",
    "# #     df_train = MA_disaster_combined.iloc[train_index]\n",
    "# #     df_test = MA_disaster_combined.iloc[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2937185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6765, 21) (778, 21)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0b5d575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incidentType\n",
      "Hurricane            2702\n",
      "Biological           1930\n",
      "Tropical Storm        465\n",
      "Flood                 462\n",
      "Fire                  437\n",
      "Severe Storm          350\n",
      "Typhoon               115\n",
      "Tornado                74\n",
      "Other                  50\n",
      "Severe Ice Storm       40\n",
      "Coastal Storm          36\n",
      "Mud/Landslide          25\n",
      "Volcanic Eruption      25\n",
      "Earthquake             22\n",
      "Dam/Levee Break        12\n",
      "Chemical                7\n",
      "Snowstorm               5\n",
      "Winter Storm            4\n",
      "Terrorist               4\n",
      "Name: count, dtype: int64 incidentType\n",
      "Hurricane           485\n",
      "Flood                92\n",
      "Earthquake           57\n",
      "Fire                 56\n",
      "Severe Storm         50\n",
      "Tornado              12\n",
      "Winter Storm          6\n",
      "Other                 5\n",
      "Severe Ice Storm      5\n",
      "Snowstorm             4\n",
      "Tropical Storm        4\n",
      "Typhoon               2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['incidentType'].value_counts(), df_test['incidentType'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "12fed912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(train_filepath)\n",
    "df_test.to_parquet(test_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a33fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "milestone2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
